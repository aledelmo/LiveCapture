/* -LICENSE-START-
 ** Copyright (c) 2018 Blackmagic Design
 **
 ** Permission is hereby granted, free of charge, to any person or organization
 ** obtaining a copy of the software and accompanying documentation covered by
 ** this license (the "Software") to use, reproduce, display, distribute,
 ** execute, and transmit the Software, and to prepare derivative works of the
 ** Software, and to permit third-parties to whom the Software is furnished to
 ** do so, all subject to the following:
 **
 ** The copyright notices in the Software and this entire statement, including
 ** the above license grant, this restriction and the following disclaimer,
 ** must be included in all copies of the Software, in whole or in part, and
 ** all derivative works of the Software, unless such copies or derivative
 ** works are solely in the form of machine-executable object code generated by
 ** a source language processor.
 **
 ** THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 ** IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 ** FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
 ** SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
 ** FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
 ** ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 ** DEALINGS IN THE SOFTWARE.
 ** -LICENSE-END-
 */
#include "LiveVideoWithOpenCV.h"
#include "com_utils.h"

#ifdef _WIN32
#include "DeckLinkAPI_i.c"
#endif

namespace
{
	bool verbose = true;

	void growRect(cv::Rect& rect, double xFactor, double yFactor)
	{
		cv::Size  deltaSize((int)(rect.width * xFactor), (int)(rect.height * yFactor));
		cv::Point offset(deltaSize.width / 2, deltaSize.height / 2);
		rect += deltaSize;
		rect -= offset;
	}

	void fillWithBlackVideo(const com_ptr<IDeckLinkMutableVideoFrame>& theFrame)
	{
		void* bytes;
		theFrame->GetBytes(&bytes);
		auto numWords = theFrame->GetRowBytes() * theFrame->GetHeight() / sizeof(uint32_t);

		// Treat as pointer to uint32_t so we can fill with a UYVY pattern for black
		auto		   words	  = reinterpret_cast<uint32_t*>(bytes);
		const uint32_t kUYVYBlack = 0x10801080;

		std::fill(words, words + numWords, kUYVYBlack);
	}

	int roundToMultipleOf(int number, int multiple)
	{
		return ((number + (multiple / 2)) / multiple) * multiple;
	}
}

void Stabilizer::addRect(const cv::Rect& rect)
{
	if (rects.size() > kNumFrames)
	{
		rects.pop_front();
	}

	rects.push_back(rect);
}

size_t Stabilizer::count() const
{
	return rects.size();
}

cv::Rect Stabilizer::average() const
{
	// Get mean centre
	double   xSum = 0.0, ySum = 0.0;
	unsigned wSum = 0, hSum = 0;

	for (const auto& r : rects)
	{
		xSum += r.x + r.width / 2.0;
		ySum += r.y + r.height / 2.0;
		wSum += r.width;
		hSum += r.height;
	}

	double centreX  = xSum / rects.size();
	double centreY  = ySum / rects.size();
	double averageW = wSum / rects.size();
	double averageH = hSum / rects.size();

	// The x coord must be rounded to the nearest multiple of 4 columns to keep UYVY colors correct
	return cv::Rect(roundToMultipleOf((int)(centreX - averageW / 2.0), 4),
					(int)std::round(centreY - averageH / 2.0),
					(int)std::round(averageW),
					(int)std::round(averageH));
}

Capture* Capture::CreateInstance(com_ptr<IDeckLinkInput> deckLinkInput, com_ptr<IDeckLinkOutput> deckLinkOutput, BMDDisplayMode displayMode)
{
	return new Capture(deckLinkInput, deckLinkOutput, displayMode);
}

void Capture::StartDeckLinkCapture()
{
	if (verbose)
		std::cout << "StartDeckLinkCapture" << std::endl;

	std::lock_guard<std::mutex> lock(m_deckLinkInputMutex);

	const BMDPixelFormat pixelFormat = bmdFormat8BitYUV;

	unsigned	 width, height;
	BMDTimeValue frameDuration;

	getWidthHeightForDisplayMode(m_displayMode, width, height, frameDuration, m_timeScale);

    m_deckLinkInput->EnableVideoInput(m_displayMode, pixelFormat, bmdVideoInputEnableFormatDetection);


    if (FAILED(m_deckLinkInput->EnableVideoInput(m_displayMode, pixelFormat, bmdVideoInputEnableFormatDetection)))
		throw std::runtime_error("Could not enable video input");

	if (FAILED(m_deckLinkOutput->EnableVideoOutput(m_displayMode, bmdVideoOutputFlagDefault)))
		throw std::runtime_error("Could not enable video output");

	for (int i = 0; i < 10; i++)
	{
		IDeckLinkMutableVideoFrame* outputFrame;
		CHECK_API(m_deckLinkOutput->CreateVideoFrame(width, height, width * 2, bmdFormat8BitYUV, bmdFrameFlagDefault, &outputFrame));

		m_outputVideoFrameQueue.push_back(outputFrame);
	}
	m_outputVideoFrameIndex = 0;

	// Create a black frame

	CHECK_API(m_deckLinkOutput->CreateVideoFrame(width, height, width * 2, bmdFormat8BitYUV, bmdFrameFlagDefault, m_blackVideoFrame.releaseAndGetAddressOf()));
	fillWithBlackVideo(m_blackVideoFrame);

	if (FAILED(m_deckLinkOutput->StartScheduledPlayback(0, m_timeScale, 1.0)))
		throw std::runtime_error("Could not start video output");

	if (FAILED(m_deckLinkInput->SetCallback(this)))
		throw std::runtime_error("Could not set callback on input");
	if (FAILED(m_deckLinkInput->StartStreams()))
		throw std::runtime_error("Could not StartStreams");
}

void Capture::StopDeckLinkCapture()
{
	if (verbose)
		std::cout << "StopDeckLinkCapture" << std::endl;
	std::lock_guard<std::mutex> lock(m_deckLinkInputMutex);
	m_deckLinkInput->StopStreams();
	m_deckLinkInput->SetCallback(nullptr);  // blocks until all callbacks have completed
	m_deckLinkInput->DisableVideoInput();
}

void Capture::setProcessMode(ProcessMode mode)
{
	m_processMode = mode;
	switch (mode)
	{
		case kProcessModeBlack: std::cout << "kProcessModeBlack\n"; break;
		case kProcessModeLoopThrough: std::cout << "kProcessModeLoopThrough\n"; break;
		case kProcessModeGrayScale: std::cout << "kProcessModeGrayScale\n"; break;
		case kProcessModeResizeFiltered: std::cout << "kProcessModeResizeFiltered\n"; break;
		case kProcessModeResizeNearest: std::cout << "kProcessModeResizeNearest\n"; break;
		case kProcessModeResizeTooSmall: std::cout << "kProcessModeResizeTooSmall\n"; break;
		case kProcessModeDetectWithAnnotation: std::cout << "kProcessModeDetectWithAnnotation\n"; break;
		case kProcessModeDetectWithAnnotationAndExtract: std::cout << "kProcessModeDetectWithAnnotationAndExtract\n"; break;
		default: std::cout << "unknown process mode\n"; break;
	}
}

void Capture::toggleProcessFlag(ProcessFlag flag)
{
	m_processFlag[flag] = !m_processFlag[flag];
	switch (flag)
	{
		case kProcessFlagResizeScale: std::cout << "ResizeScale = " << m_processFlag[flag] << std::endl; break;
		case kProcessFlagFilterQuality: std::cout << "FilterQuality = " << m_processFlag[flag] << std::endl; break;
	}
}

void Capture::setClassifier(Detector detector)
{
	std::cout << "Detector = " << (int)detector << std::endl;
	m_detector = detector;
}

Capture::Capture(com_ptr<IDeckLinkInput> deckLinkInput, com_ptr<IDeckLinkOutput> deckLinkOutput, BMDDisplayMode displayMode)
	: m_displayMode(displayMode)
	, m_refCount(1)
	, m_outputVideoFrameIndex(0)
	, m_timeScale(0)
	, m_processMode(kProcessModeDetectWithAnnotationAndExtract)
	, m_processFlag( { { kProcessFlagResizeScale, false }, { kProcessFlagFilterQuality, false } } )
	, m_detector(Detector::FrontalFace)
	, m_classifiers ( {
		{ Detector::FrontalFace, cv::String(CLASSIFIER_BASE_PATH "/haarcascades/haarcascade_frontalface_default.xml") },
		{ Detector::ProfileFace, cv::String(CLASSIFIER_BASE_PATH "/haarcascades/haarcascade_profileface.xml") },
		{ Detector::UpperBody, cv::String(CLASSIFIER_BASE_PATH "/haarcascades/haarcascade_upperbody.xml") },
		{ Detector::FullBody, cv::String(CLASSIFIER_BASE_PATH "/haarcascades/haarcascade_fullbody.xml") },
		{ Detector::Smile, cv::String(CLASSIFIER_BASE_PATH "/haarcascades/haarcascade_smile.xml") },
		{ Detector::Eye, cv::String(CLASSIFIER_BASE_PATH "/haarcascades/haarcascade_eye.xml") } } )
{
	m_deckLinkInput = deckLinkInput;
	m_deckLinkOutput = deckLinkOutput;

#ifndef _WIN32
	m_deckLinkVideoConversion = CreateVideoConversionInstance();
#else
	CoCreateInstance(CLSID_CDeckLinkVideoConversion, NULL, CLSCTX_ALL, IID_IDeckLinkVideoConversion, (void**)m_deckLinkVideoConversion.releaseAndGetAddressOf());
#endif
}

void Capture::getWidthHeightForDisplayMode(BMDDisplayMode displayMode, uint32_t& width, uint32_t& height, BMDTimeValue& frDuration, BMDTimeScale& frScale)
{
	IDeckLinkDisplayModeIterator* displayModeIterator = nullptr;
	IDeckLinkDisplayMode*		  deckLinkDisplayMode = nullptr;
	CHECK_API(m_deckLinkOutput->GetDisplayModeIterator(&displayModeIterator));

	while (displayModeIterator->Next(&deckLinkDisplayMode) == S_OK)
	{
		BMDTimeValue frameRateDuration;
		BMDTimeScale frameRateScale;

		deckLinkDisplayMode->GetFrameRate(&frameRateDuration, &frameRateScale);

		if (deckLinkDisplayMode->GetDisplayMode() == displayMode)
		{
			CHECK_API(deckLinkDisplayMode->GetFrameRate(&frameRateDuration, &frameRateScale));
			width		= (uint32_t)deckLinkDisplayMode->GetWidth();
			height		= (uint32_t)deckLinkDisplayMode->GetHeight();
			frDuration	= frameRateDuration;
			frScale		= frameRateScale;
			return;
		}
	}
	throw std::runtime_error("getWidthHeightForDisplayMode: did not find mode " + std::to_string(displayMode));
}

HRESULT Capture::QueryInterface(REFIID iid, LPVOID* ppv)
{
	HRESULT			result = S_OK;

	if (iid == IID_IUnknown)
	{
		*ppv = this;
		AddRef();
	}
	else if (iid == IID_IDeckLinkInputCallback)
	{
		*ppv = static_cast<IDeckLinkInputCallback*>(this);
		AddRef();
	}
	else if (iid == IID_IDeckLinkVideoOutputCallback)
	{
		*ppv = static_cast<IDeckLinkVideoOutputCallback*>(this);
		AddRef();
	}
	else
	{
		result = E_NOINTERFACE;
	}

	return result;
}

ULONG Capture::AddRef()
{
	return ++m_refCount;
}

ULONG Capture::Release()
{
	ULONG refCount = --m_refCount;
	if (refCount == 0)
		delete this;

	return refCount;
}

HRESULT Capture::VideoInputFormatChanged(BMDVideoInputFormatChangedEvents /* unused */, IDeckLinkDisplayMode* newDisplayMode, BMDDetectedVideoInputFormatFlags /* unused */)
{
	const BMDDisplayMode newBMDDisplayMode = newDisplayMode->GetDisplayMode();
	const BMDPixelFormat pixelFormat	   = bmdFormat8BitYUV;

	{
		std::lock_guard<std::mutex> lock(m_deckLinkInputMutex);

		CHECK_API(m_deckLinkInput->StopStreams());
		CHECK_API(m_deckLinkInput->EnableVideoInput(newBMDDisplayMode, pixelFormat, bmdVideoInputEnableFormatDetection));
		CHECK_API(m_deckLinkInput->StartStreams());
	}

	return S_OK;
}

HRESULT Capture::ScheduledFrameCompleted(IDeckLinkVideoFrame* completedFrame, BMDOutputFrameCompletionResult /* unused */)
{
	completedFrame->Release();
	return S_OK;
}

HRESULT Capture::ScheduledPlaybackHasStopped()
{
	// NO-OP
	return S_OK;
}

HRESULT Capture::VideoInputFrameArrived(IDeckLinkVideoInputFrame* videoFrame, IDeckLinkAudioInputPacket* /* unused */)
{
	// Ignore empty video frames
	if (videoFrame == nullptr)
		return S_OK;

	BMDFrameFlags flags = videoFrame->GetFlags();

	// Detect dropped frames by comparing the stream time of this frame with the previously captured frame.
	// If we're too slow processing a frame DeckLink we will eventually drop frames and there
	// will be a difference greater than one frame difference between frame arrivals.
	BMDTimeValue capturedFrameNumber;
	BMDTimeValue frameDuration;
	CHECK_API(videoFrame->GetStreamTime(&capturedFrameNumber, &frameDuration, m_timeScale));

	// For black input frames, or for 'pass-through' mode, just schedule input frame as output frame
	if (flags & bmdFrameHasNoInputSource || m_processMode == kProcessModeLoopThrough)
	{
		m_deckLinkOutput->ScheduleVideoFrame(videoFrame, capturedFrameNumber, frameDuration, m_timeScale);
	}
	else
	{
		void* data;
		videoFrame->GetBytes(&data);

		const auto width	= (int32_t)videoFrame->GetWidth();
		const auto height   = (int32_t)videoFrame->GetHeight();
		const auto numBytes = videoFrame->GetRowBytes() * height;

		auto startTime = std::chrono::high_resolution_clock::now();

		cv::Mat  frameCPU(height, width, CV_8UC2, data);
		cv::UMat image(height, width, CV_8UC1);

		if (m_processMode != kProcessModeBlack)
			cvtColor(frameCPU, image, cv::COLOR_YUV2GRAY_UYVY);

		// Copy the input frame - we will compose using it
		IDeckLinkMutableVideoFrame* outputFrame = m_outputVideoFrameQueue[m_outputVideoFrameIndex];

		// Use the DeckLinkAPI video frame conversion interface to ensure the output frame is in the expected 8-bit YUV format
		m_deckLinkVideoConversion->ConvertFrame(videoFrame, outputFrame);

		void* outData = nullptr;
		CHECK_API(outputFrame->GetBytes(&outData));
		memcpy(outData, data, (size_t)numBytes);

		cv::Mat outputUYVY(height, width, CV_8UC2, outData);

		const double scaleFactor = 1.0 / 8.0;
		if (m_processMode == kProcessModeBlack)
		{
			// Nothing to process for black frame
		}
		else if (m_processMode == kProcessModeGrayScale)
		{
			cv::UMat			  UV(height, width, CV_8UC1, cv::Scalar(128));
			std::vector<cv::UMat> channels{UV, image};
			merge(channels, outputUYVY);
		}
		else if (m_processMode == kProcessModeResizeFiltered)
		{
			cv::resize(image, image, cv::Size(), scaleFactor, scaleFactor, cv::INTER_AREA);
			cv::resize(image, image, cv::Size(), 1.0 / scaleFactor, 1.0 / scaleFactor, cv::INTER_LINEAR);

			cv::UMat			  UV(height, width, CV_8UC1, cv::Scalar(128));
			std::vector<cv::UMat> channels{UV, image};
			merge(channels, outputUYVY);
		}
		else if (m_processMode == kProcessModeResizeNearest)
		{
			cv::resize(image, image, cv::Size(), scaleFactor, scaleFactor, cv::INTER_NEAREST);
			cv::resize(image, image, cv::Size(), 1.0 / scaleFactor, 1.0 / scaleFactor, cv::INTER_NEAREST);

			cv::UMat			  UV(height, width, CV_8UC1, cv::Scalar(128));
			std::vector<cv::UMat> channels{UV, image};
			merge(channels, outputUYVY);
		}
		else if (m_processMode == kProcessModeResizeTooSmall)
		{
			const double scaleDownFactor = 1.0 / 24.0;

			cv::resize(image, image, cv::Size(), scaleDownFactor, scaleDownFactor, cv::INTER_NEAREST);
			blur(image, image, cv::Size(5, 5));
			cv::resize(image, image, cv::Size(), 1.0 / scaleDownFactor, 1.0 / scaleDownFactor, cv::INTER_NEAREST);

			cv::UMat			  UV(height, width, CV_8UC1, cv::Scalar(128));
			std::vector<cv::UMat> channels{UV, image};
			merge(channels, outputUYVY);
		}
		else if (m_processMode == kProcessModeDetectWithAnnotation)
		{
			// Downscale using suitable filter
			const double				 scaleDownFactor = m_processFlag[kProcessFlagResizeScale] ? 1.0 / 16.0 : 1.0 / 8.0;
			const cv::InterpolationFlags filter			 = m_processFlag[kProcessFlagFilterQuality] ? cv::INTER_NEAREST : cv::INTER_AREA;
			cv::resize(image, image, cv::Size(), scaleDownFactor, scaleDownFactor, filter);

			std::vector<cv::Rect> faces;
			std::vector<int>	  numDetections;

			// Detect faces
			m_classifiers.at(m_detector).detectMultiScale(image, faces, numDetections, 1.1, 2, CV_HAAR_SCALE_IMAGE, cv::Size(30, 30));

			if (! faces.empty())
			{
				double scale = 1.0 / scaleDownFactor;

				// Faces were detected - draw a rectangle around each and label with the numDetections "score"
				for (size_t i = 0; i < faces.size(); i++)
				{
					rectangle(outputUYVY, faces[i].tl() * scale, faces[i].br() * scale, cv::Scalar(128, 255));  // thin white for others
					putText(outputUYVY, std::to_string(numDetections[i]), cv::Point(faces[i].x, faces[i].br().y + 20) * scale, cv::FONT_HERSHEY_SIMPLEX, 7., cv::Scalar(128, 255), 2);
				}
			}

			// Draw the elapsed processing time in the bottom right corner
			auto microsec = std::chrono::duration_cast<std::chrono::nanoseconds>(std::chrono::high_resolution_clock::now() - startTime).count() / 1000000.0;
			putText(outputUYVY, std::to_string(microsec) + "ms", cv::Point(width - 800, height - 100), cv::FONT_HERSHEY_SIMPLEX, 7., cv::Scalar(128, 255), 2);
		}
		else
		{
			// Downscale using suitable filter
			cv::resize(image, image, cv::Size(), scaleFactor, scaleFactor, cv::INTER_AREA);

			std::vector<cv::Rect> faces;
			std::vector<int>	  numDetections;

			// Detect faces
			m_classifiers.at(m_detector).detectMultiScale(image, faces, numDetections, 1.1, 2, CV_HAAR_SCALE_IMAGE, cv::Size(30, 30));

			if (! faces.empty())
			{
				double scale	 = 1.0 / scaleFactor;
				auto   bestIndex = std::distance(numDetections.begin(),
												 std::max_element(numDetections.begin(), numDetections.end()));

				// Faces were detected - draw a rectangle around each and label with the numDetections "score"
				for (size_t i = 0; i < faces.size(); i++)
				{
					if (i == bestIndex)
						rectangle(outputUYVY, faces[i].tl() * scale, faces[i].br() * scale, cv::Scalar(0, 255), 8);	// thick green border for best face
					else
						rectangle(outputUYVY, faces[i].tl() * scale, faces[i].br() * scale, cv::Scalar(128, 255));	// thin white for others

					putText(outputUYVY, std::to_string(numDetections[i]), cv::Point(faces[i].x, faces[i].br().y + 20) * scale, cv::FONT_HERSHEY_SIMPLEX, 7., cv::Scalar(128, 255), 2);
				}

				auto bestFace = cv::Rect(faces[bestIndex].tl() * scale, faces[bestIndex].br() * scale);

				// Grow face a little to give more of the person's face
				growRect(bestFace, 0.6, 0.8);

				// Clip to frame dimensions (may be outside frame after grow step above)
				bestFace = bestFace & cv::Rect(0, 0, width - 1, height - 1);

				// Add faces to stabilizer
				if (m_stabilizer.count() < 10 || numDetections[bestIndex] >= 15)
					m_stabilizer.addRect(bestFace);
			}

			if (m_stabilizer.count() >= 10)
			{
				const cv::Rect stabRect = m_stabilizer.average();
				const cv::Rect clipRect(0, 0, width - 1, height - 1);
				cv::Mat		   faceInOrig = frameCPU(stabRect & clipRect);

				// Copy face to top left corner of screen
				faceInOrig.copyTo(outputUYVY(cv::Rect(0, 0, faceInOrig.cols, faceInOrig.rows)));

				cv::Mat decklinkOutput(height, width, CV_8UC2, outData);
				outputUYVY.copyTo(decklinkOutput);
			}

			// Draw the elapsed processing time in the bottom right corner
			auto microsec = std::chrono::duration_cast<std::chrono::nanoseconds>(std::chrono::high_resolution_clock::now() - startTime).count() / 1000000.0;
			putText(outputUYVY, std::to_string(microsec) + "ms", cv::Point(width - 800, height - 100), cv::FONT_HERSHEY_SIMPLEX, 7., cv::Scalar(128, 255), 2);
		}

		if (m_processMode == kProcessModeBlack)
		{
			auto hr = m_deckLinkOutput->ScheduleVideoFrame(m_blackVideoFrame.get(), capturedFrameNumber, frameDuration, m_timeScale);
			if (FAILED(hr) && hr != E_OUTOFMEMORY)
				throw std::runtime_error("schedule failed");
		}
		else
		{
			auto hr = m_deckLinkOutput->ScheduleVideoFrame(outputFrame, capturedFrameNumber, frameDuration, m_timeScale);
			if (FAILED(hr) && hr != E_OUTOFMEMORY)
				throw std::runtime_error("schedule failed");
		}
	}

	m_outputVideoFrameIndex++;
	if (m_outputVideoFrameIndex >= m_outputVideoFrameQueue.size())
		m_outputVideoFrameIndex = 0;

	return S_OK;
}

int usage()
{
	using namespace std;

	cout << "Usage: LiveVideoWithOpenCV [options]" << endl;
	cout << endl;
	cout << "	-q					Disable verbose logging" << endl;
	cout << endl;

	return 1;
}

int main(int argc, char* argv[])
{
	for (int i = 1; i < argc; i++)
	{
		if (strcmp(argv[i], "-h") == 0 || strcmp(argv[i], "--help") == 0)
		{
			usage();
			return 0;
		}
		else if (strcmp(argv[i], "-q") == 0 || strcmp(argv[i], "--quiet") == 0)
		{
			verbose = false;
		}
		else
			return usage();
	}

	try
	{
		DeviceManager deviceManager;
		auto		  deckLinkInput  = deviceManager.getDeckLinkInput();
		auto		  deckLinkOutput = deviceManager.getDeckLinkOutput();

		auto		  capture		 = Capture::CreateInstance(deckLinkInput, deckLinkOutput, bmdModeHD1080p6000);

		capture->StartDeckLinkCapture();

		// Display an image with instructions for the key press commands
		int		lineNumber = 1;
		cv::Mat image	  = cv::Mat::zeros(776, 800, CV_8UC1);
		for (const auto text : {
				 "Command keys:",
				 "  0 - Black frame",
				 "  1 - Live video loop through",
				 "  2 - Greyscale",
				 "  3 - Resize well filtered",
				 "  4 - Resize nearest neighbor filtered",
				 "  5 - Resize too small",
				 "  6 - Detect and annotate faces",
				 "  7 - Detect and annotate faces with extract",
				 "  z - Toggle resize scale",
				 "  x - Toggle filter quality",
				 "  a - Frontal Face",
				 "  s - Profile Face",
				 "  d - Upper Body",
				 "  f - Full Body",
				 "  g - Smile",
				 "  h - Eye",
				 "  q - Quit",
			 })
		{
			const int lineSpacing = 40;
			putText(image, text, cv::Point(20, (lineNumber++) * lineSpacing), cv::FONT_HERSHEY_SIMPLEX, 1.0, cv::Scalar(235));
		}
		imshow("Live Video Command Window", image);

		// Loop forever, processing key presses
		int keyCode;
		while ((keyCode = cv::waitKey()) != 'q')
		{
			switch (keyCode)
			{
				case '0': capture->setProcessMode(kProcessModeBlack); break;
				case '1': capture->setProcessMode(kProcessModeLoopThrough); break;
				case '2': capture->setProcessMode(kProcessModeGrayScale); break;
				case '3': capture->setProcessMode(kProcessModeResizeFiltered); break;
				case '4': capture->setProcessMode(kProcessModeResizeNearest); break;
				case '5': capture->setProcessMode(kProcessModeResizeTooSmall); break;
				case '6': capture->setProcessMode(kProcessModeDetectWithAnnotation); break;
				case '7': capture->setProcessMode(kProcessModeDetectWithAnnotationAndExtract); break;
				case 'z': capture->toggleProcessFlag(kProcessFlagResizeScale); break;
				case 'x': capture->toggleProcessFlag(kProcessFlagFilterQuality); break;
				case 'a': capture->setClassifier(Detector::FrontalFace); break;
				case 's': capture->setClassifier(Detector::ProfileFace); break;
				case 'd': capture->setClassifier(Detector::UpperBody); break;
				case 'f': capture->setClassifier(Detector::FullBody); break;
				case 'g': capture->setClassifier(Detector::Smile); break;
				case 'h': capture->setClassifier(Detector::Eye); break;
				default: break;
			}
		}

		// Quit was pressed so stop capture
		capture->StopDeckLinkCapture();
	}
	catch (const std::runtime_error& e)
	{
		std::cerr << "VideoStreaming failed: " << e.what() << std::endl;
		return 1;
	}

	return 0;
}
